[{"content":"This guide details how to set up a lightweight Kubernetes (K3s) cluster using LXD and LXC containers on an Ubuntu 24.04 server, followed by deploying a basic NGINX test service with NGINX Ingress. These steps were tested on an Oracle Cloud Free Tier instance named maata-paarvati with 4 CPUs and 24 GB RAM.\nPrerequisites An Ubuntu 24.04 server (e.g., Oracle Cloud Free Tier instance). Root or sudo access. Network configured to allow TCP ports 80, 443, and 6443. 1. Install LXD Install LXD via Snap:\nsudo snap install lxd Initialize LXD:\nUse default settings for a simple setup:\nsudo lxd init --auto This creates a dir storage pool (default) and a bridge network (lxdbr0).\nAdd User to LXD Group:\nsudo usermod -aG lxd ubuntu newgrp lxd Verify LXD:\nlxc list Should display an empty table initially. 2. Create a Kubernetes Profile for LXC Containers Create the Profile Configuration:\nSave this as k8s-profile.yaml:\nconfig: limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2GB limits.memory.swap: \u0026#34;false\u0026#34; linux.kernel_modules: ip_tables,ip6_tables,nf_nat,overlay,br_netfilter raw.lxc: | lxc.apparmor.profile=unconfined lxc.cap.drop= lxc.cgroup.devices.allow=a lxc.mount.auto=proc:rw sys:rw security.privileged: \u0026#34;true\u0026#34; security.nesting: \u0026#34;true\u0026#34; security.syscalls.intercept.mknod: \u0026#34;true\u0026#34; security.syscalls.intercept.setxattr: \u0026#34;true\u0026#34; description: LXD profile for Kubernetes devices: eth0: name: eth0 nictype: bridged parent: lxdbr0 type: nic kmsg: path: /dev/kmsg source: /dev/console type: unix-char root: path: / pool: default type: disk name: k8s used_by: [] This matches your lxc config show kmaster --expanded output, excluding volatile and proxy settings added later.\nApply the Profile:\nlxc profile create k8s lxc profile edit k8s \u0026lt; k8s-profile.yaml Verify Profile:\nlxc profile show k8s 3. Launch LXC Containers with Ubuntu 24.04 Initialize Containers:\nlxc init ubuntu:24.04 kmaster --profile k8s lxc init ubuntu:24.04 kworker1 --profile k8s lxc init ubuntu:24.04 kworker2 --profile k8s Start Containers:\nlxc start kmaster lxc start kworker1 lxc start kworker2 Verify Containers:\nlxc list Example output (IPs will vary):\n+---------+---------+----------------------+------+-----------+-----------+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +---------+---------+----------------------+------+-----------+-----------+ | kmaster | RUNNING | 10.177.108.101 (eth0)| | CONTAINER | 0 | | kworker1| RUNNING | 10.177.108.79 (eth0) | | CONTAINER | 0 | | kworker2| RUNNING | 10.177.108.57 (eth0) | | CONTAINER | 0 | +---------+---------+----------------------+------+-----------+-----------+ 4. Install K3s and Join the Cluster Prepare kmaster:\nlxc exec kmaster -- bash -c \u0026#34;apt update \u0026amp;\u0026amp; apt install -y curl iptables\u0026#34; Install K3s on kmaster:\nDisable Traefik to use NGINX Ingress later:\nlxc exec kmaster -- bash -c \u0026#34;curl -sfL https://get.k3s.io | sh -s - server --disable=traefik --write-kubeconfig-mode 644\u0026#34; Get Node Token:\nlxc exec kmaster -- cat /var/lib/rancher/k3s/server/node-token Save the token (e.g., K10...::server:...). Prepare and Join Worker Nodes:\nFor kworker1:\nlxc exec kworker1 -- bash -c \u0026#34;apt update \u0026amp;\u0026amp; apt install -y curl iptables\u0026#34; lxc exec kworker1 -- bash -c \u0026#34;curl -sfL https://get.k3s.io | K3S_URL=https://10.177.108.101:6443 K3S_TOKEN=\u0026lt;node-token\u0026gt; sh -\u0026#34; For kworker2:\nlxc exec kworker2 -- bash -c \u0026#34;apt update \u0026amp;\u0026amp; apt install -y curl iptables\u0026#34; lxc exec kworker2 -- bash -c \u0026#34;curl -sfL https://get.k3s.io | K3S_URL=https://10.177.108.101:6443 K3S_TOKEN=\u0026lt;node-token\u0026gt; sh -\u0026#34; Verify Cluster:\nlxc exec kmaster -- k3s kubectl get nodes Expected:\nNAME STATUS ROLES AGE VERSION kmaster Ready control-plane,master 5m v1.32.3+k3s1 kworker1 Ready \u0026lt;none\u0026gt; 2m v1.32.3+k3s1 kworker2 Ready \u0026lt;none\u0026gt; 1m v1.32.3+k3s1 Set Up Host Access:\nInstall kubectl on the host:\nsudo snap install kubectl --classic Copy kubeconfig:\nlxc file pull kmaster/etc/rancher/k3s/k3s.yaml ~/.kube/config Edit ~/.kube/config, replace 127.0.0.1 with 10.177.108.101:\nserver: https://10.177.108.101:6443 Test:\nkubectl get nodes Add Port Forwarding for K3s API:\nsudo iptables -t nat -A PREROUTING -p tcp --dport 6443 -j DNAT --to-destination 10.177.108.101:6443 sudo iptables -t nat -A POSTROUTING -j MASQUERADE sudo sysctl -w net.ipv4.ip_forward=1 5. Set Up NGINX Ingress and Deploy a Test Service Install Helm:\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 sudo bash get_helm.sh Install NGINX Ingress:\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install ingress-nginx ingress-nginx/ingress-nginx \\ --namespace ingress-nginx \\ --create-namespace \\ --set controller.service.type=LoadBalancer Add Proxy Devices for HTTP/HTTPS:\nBased on your lxc config show kmaster --expanded:\nlxc config device add kmaster http proxy listen=tcp:0.0.0.0:80 connect=tcp:127.0.0.1:80 lxc config device add kmaster https proxy listen=tcp:0.0.0.0:443 connect=tcp:127.0.0.1:443 Add Port Forwarding for HTTP/HTTPS:\nGet NGINX service IP:\nkubectl get svc -n ingress-nginx Example: 10.177.108.x. Forward ports:\nsudo iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination 10.177.108.x:80 sudo iptables -t nat -A PREROUTING -p tcp --dport 443 -j DNAT --to-destination 10.177.108.x:443 sudo iptables -t nat -A POSTROUTING -j MASQUERADE Deploy a Basic NGINX Test Service:\nCreate nginx-test.yaml:\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-test namespace: default spec: replicas: 1 selector: matchLabels: app: nginx-test template: metadata: labels: app: nginx-test spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-test namespace: default spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: app: nginx-test --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-test namespace: default annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: nginx-test.local http: paths: - path: / pathType: Prefix backend: service: name: nginx-test port: number: 80 Apply:\nkubectl apply -f nginx-test.yaml Test the NGINX Service:\nUpdate /etc/hosts on your local machine (temporary workaround since nginx-test.local isn’t a real domain):\necho \u0026#34;\u0026lt;public-ip\u0026gt; nginx-test.local\u0026#34; | sudo tee -a /etc/hosts Use your Oracle Cloud instance’s public IP (e.g., curl ifconfig.me). Test:\ncurl http://nginx-test.local Should return the NGINX welcome page HTML. Conclusion You now have a K3s cluster running inside LXC containers managed by LXD, with NGINX Ingress and a basic NGINX test service accessible at http://nginx-test.local. This setup leverages your Oracle Cloud Free Tier instance efficiently, with Traefik disabled and NGINX handling ingress traffic.\n","permalink":"https://blog.rawlani.com/posts/setting-up-lxd-lxc-and-k3s-with-a-basic-nginx-test-service/","summary":"\u003cp\u003eThis guide details how to set up a lightweight Kubernetes (K3s) cluster using LXD and LXC containers on an Ubuntu 24.04 server, followed by deploying a basic NGINX test service with NGINX Ingress. These steps were tested on an Oracle Cloud Free Tier instance named \u003ccode\u003emaata-paarvati\u003c/code\u003e with 4 CPUs and 24 GB RAM.\u003c/p\u003e\n\u003ch2 id=\"prerequisites\"\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAn Ubuntu 24.04 server (e.g., Oracle Cloud Free Tier instance).\u003c/li\u003e\n\u003cli\u003eRoot or sudo access.\u003c/li\u003e\n\u003cli\u003eNetwork configured to allow TCP ports 80, 443, and 6443.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-install-lxd\"\u003e1. Install LXD\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInstall LXD via Snap\u003c/strong\u003e:\u003c/p\u003e","title":"Setting Up Lxd Lxc and K3s With a Basic Nginx Test Service"},{"content":"LXC vs. LXD: Understanding the Differences and Choosing the Right Tool Introduction Linux containers have revolutionized how we deploy and manage applications, offering a lightweight alternative to virtual machines. But with tools like LXC (Linux Containers) and LXD (Linux Container Daemon), choosing the right one can be confusing. Did you know that while both are based on Linux kernel features, LXD builds on LXC for a more user-friendly experience? In this post, we\u0026rsquo;ll explore what LXC and LXD are, how they differ, and when to use each. By the end, you\u0026rsquo;ll have a clear understanding to make an informed decision for your projects.\nInsert an image here showing how containers share the host kernel while providing isolated environments.\nWhat are Linux Containers? Linux containers are a form of OS-level virtualization, allowing multiple isolated applications to run on a single host. They share the host\u0026rsquo;s kernel but have their own file systems and processes, making them more efficient than VMs. LXC and LXD leverage kernel features like namespaces and cgroups for isolation and resource control.\nUnderstanding LXC LXC is a low-level interface for Linux kernel containment, providing tools to create and manage containers directly. It uses commands like lxc-create and lxc-start for operations.\nExample: sudo lxc-create -n mycontainer -t ubuntu sudo lxc-start -n mycontainer LXC is lightweight and performant but requires deeper Linux knowledge.\nUnderstanding LXD LXD builds on LXC, adding a daemon for easier management via a REST API. It supports advanced features like snapshots and live migration, with commands like lxc launch.\nExample: sudo lxd init sudo lxc launch ubuntu:22.04 mycontainer LXD is user-friendly, ideal for beginners and large-scale setups.\nComparing LXC and LXD Here\u0026rsquo;s a comparison to help you decide:\nAspect LXC LXD Ease of Use Complex, for experienced users User-friendly, beginner-friendly Features Basic, no snapshots Advanced, includes snapshots, migration Performance Higher, minimal overhead Slight overhead, still efficient Scalability Limited, manual management Designed for large-scale, clustering Insert an image here comparing LXC and LXD architectures.\nWhen to Use LXC Use LXC for:\nResource-constrained environments. Users needing fine-grained control. Simple setups with basic needs. When to Use LXD Use LXD for:\nEase of management and advanced features. Large-scale deployments or cloud setups. Beginners or teams needing scalability. Community and Ecosystem LXC has a smaller community, documented at Linux Containers. LXD, under Canonical, has extensive resources at Ubuntu LXD Docs.\nConclusion In summary, LXC is lightweight but complex, while LXD is user-friendly with advanced features. Now you\u0026rsquo;re equipped to choose based on your needs. Share your experiences with LXC or LXD in the comments below!\n","permalink":"https://blog.rawlani.com/posts/lxc-vs-lxd/","summary":"\u003ch1 id=\"lxc-vs-lxd-understanding-the-differences-and-choosing-the-right-tool\"\u003eLXC vs. LXD: Understanding the Differences and Choosing the Right Tool\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eLinux containers have revolutionized how we deploy and manage applications, offering a lightweight alternative to virtual machines. But with tools like \u003cem\u003eLXC (Linux Containers)\u003c/em\u003e and \u003cem\u003eLXD (Linux Container Daemon)\u003c/em\u003e, choosing the right one can be confusing. \u003cem\u003eDid you know that while both are based on Linux kernel features, LXD builds on LXC for a more user-friendly experience?\u003c/em\u003e In this post, we\u0026rsquo;ll explore what LXC and LXD are, how they differ, and when to use each. By the end, you\u0026rsquo;ll have a clear understanding to make an informed decision for your projects.\u003c/p\u003e","title":"LXC vs. LXD: Understanding the Differences and Choosing the Right Tool"},{"content":"Installing YOURLS on Kubernetes This guide walks through the process of deploying YOURLS (Your Own URL Shortener) on a Kubernetes cluster using Helm.\nPrerequisites A working Kubernetes cluster Helm installed on your machine Basic knowledge of Kubernetes concepts A domain name pointed to your cluster\u0026rsquo;s ingress controller (we\u0026rsquo;ll use go.example.com in this guide) Installation Steps 1. Create a Namespace for YOURLS First, create a dedicated namespace for YOURLS:\nkubectl create namespace yourls 2. Configure Values for YOURLS Create a values.yaml file with the following configuration:\nyourls: username: \u0026#34;admin\u0026#34; # Change to your desired admin username password: \u0026#34;securepassword\u0026#34; # Change to a secure password cookieKey: \u0026#34;randomsecurestring\u0026#34; # Use a random string for cookie encryption # The following two settings are critical for proper site URL configuration scheme: \u0026#34;https\u0026#34; domain: \u0026#34;go.example.com\u0026#34; mariadb: enabled: true # Use built-in MariaDB auth: username: \u0026#34;yourlsdb\u0026#34; # Database username password: \u0026#34;dbpassword\u0026#34; # Database password database: \u0026#34;yourls\u0026#34; # Database name rootPassword: \u0026#34;rootpass\u0026#34; # Root password for MariaDB persistence: enabled: true # Enable persistent storage 3. Install YOURLS using Helm Deploy YOURLS using the Helm chart with your custom values:\nhelm upgrade -f values.yaml --install yourls yourls/yourls --namespace yourls 4. Set Up TLS with cert-manager Install cert-manager to handle TLS certificates:\nkubectl create namespace cert-manager helm install cert-manager bitnami/cert-manager --namespace cert-manager --set installCRDs=true Create a ClusterIssuer for Let\u0026rsquo;s Encrypt:\n# letsencrypt-prod.yaml apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: admin@example.com # Change to your email privateKeySecretRef: name: letsencrypt-prod solvers: - http01: ingress: class: nginx Apply the ClusterIssuer:\nkubectl apply -f letsencrypt-prod.yaml 5. Configure Ingress for YOURLS Create an ingress configuration for YOURLS:\n# yourls-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: yourls namespace: yourls annotations: cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-prod\u0026#34; spec: ingressClassName: nginx tls: - hosts: - go.example.com secretName: go-example-com-tls rules: - host: go.example.com http: paths: - path: / pathType: Prefix backend: service: name: yourls port: number: 80 Apply the ingress configuration:\nkubectl apply -f yourls-ingress.yaml Testing Your Installation 1. Verify Pod Status Check that the YOURLS and MariaDB pods are running:\nkubectl get pods -n yourls Expected output:\nNAME READY STATUS RESTARTS AGE yourls-xxxxxxxxxx-xxxxx 1/1 Running 0 3m yourls-mariadb-0 1/1 Running 0 3m 2. Check Environment Variables Verify that the YOURLS_SITE environment variable is set correctly:\nkubectl exec -it -n yourls deploy/yourls -- env | grep YOURLS_SITE Expected output:\nYOURLS_SITE=https://go.example.com 3. Test TLS Certificate Verify that the SSL certificate is properly issued:\nkubectl get certificate -n yourls Test the HTTPS connection:\ncurl -Iv https://go.example.com You should see a successful HTTPS response with a valid certificate.\n4. Access YOURLS Admin Interface Open your browser and navigate to https://go.example.com/admin to access the YOURLS admin interface.\nLogin using the credentials you specified in the values.yaml file.\nTroubleshooting YOURLS_SITE Not Set Correctly If the YOURLS_SITE environment variable is not being set correctly, make sure you\u0026rsquo;ve configured both scheme and domain in your values.yaml:\nyourls: scheme: \u0026#34;https\u0026#34; domain: \u0026#34;go.example.com\u0026#34; Ingress Not Working Check the status of your ingress:\nkubectl describe ingress yourls -n yourls Ensure your DNS records are correctly pointing to your Kubernetes cluster\u0026rsquo;s ingress controller.\nCertificate Issues Check certificate status:\nkubectl describe certificate go-example-com-tls -n yourls Look for any errors in the cert-manager logs:\nkubectl logs -n cert-manager -l app=cert-manager Uninstallation To completely remove YOURLS from your cluster:\n# Uninstall the YOURLS Helm release helm uninstall yourls -n yourls # Delete any persistent volume claims kubectl delete pvc --all -n yourls # Delete the namespace kubectl delete namespace yourls Optional: Remove cert-manager if it\u0026rsquo;s no longer needed:\nhelm uninstall cert-manager -n cert-manager kubectl delete namespace cert-manager Notes Remember to back up your MariaDB data before making major changes to your deployment For production use, consider using external database services instead of the bundled MariaDB Keep your Kubernetes cluster and YOURLS installation updated to maintain security ","permalink":"https://blog.rawlani.com/posts/yourls-k8s-installation-guide/","summary":"\u003ch1 id=\"installing-yourls-on-kubernetes\"\u003eInstalling YOURLS on Kubernetes\u003c/h1\u003e\n\u003cp\u003eThis guide walks through the process of deploying YOURLS (Your Own URL Shortener) on a Kubernetes cluster using Helm.\u003c/p\u003e\n\u003ch2 id=\"prerequisites\"\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA working Kubernetes cluster\u003c/li\u003e\n\u003cli\u003eHelm installed on your machine\u003c/li\u003e\n\u003cli\u003eBasic knowledge of Kubernetes concepts\u003c/li\u003e\n\u003cli\u003eA domain name pointed to your cluster\u0026rsquo;s ingress controller (we\u0026rsquo;ll use \u003ccode\u003ego.example.com\u003c/code\u003e in this guide)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"installation-steps\"\u003eInstallation Steps\u003c/h2\u003e\n\u003ch3 id=\"1-create-a-namespace-for-yourls\"\u003e1. Create a Namespace for YOURLS\u003c/h3\u003e\n\u003cp\u003eFirst, create a dedicated namespace for YOURLS:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ekubectl create namespace yourls\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2-configure-values-for-yourls\"\u003e2. Configure Values for YOURLS\u003c/h3\u003e\n\u003cp\u003eCreate a \u003ccode\u003evalues.yaml\u003c/code\u003e file with the following configuration:\u003c/p\u003e","title":"Yourls K8s Installation Guide"}]